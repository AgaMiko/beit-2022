{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_pytorch_lighnting_T5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ygp8tzwUMfOc"
      },
      "source": [
        "# Jokes Generation with Text-To-Text Transfer Transformer \n",
        "Author: Agnieszka Mikołajczyk\n",
        "\n",
        "> *What's the best part of a pregnancy joke? The delivery.*\n",
        "\n",
        "~ Kaggle joke dataset\n",
        "\n",
        "Let's train the model to tell jokes!\n",
        "\n",
        "![](https://i.pinimg.com/originals/a5/cd/55/a5cd552a3aff2fc86fb99815bf970580.jpg)\n",
        "\n",
        "# T5: Text-To-Text Transfer Transformer\n",
        "\n",
        "T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: translate English to German: …, for summarization: summarize: \n",
        "\n",
        "T5 uses relative scalar embeddings. Encoder input padding can be done on the left and on the right.\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*oPH8tAGqu3aUp6qjMtqcHg.png)\n",
        "\n",
        "More: https://huggingface.co/docs/transformers/model_doc/t5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install pytorch_lightning\n",
        "!pip install SentencePiece"
      ],
      "metadata": {
        "id": "3rsDXD2uO-qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UdHBdInGfSD"
      },
      "source": [
        "Download kaggle joke dataset: https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes/download\n",
        "\n",
        "Upload it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XeNn4XdMeGb"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4T2CLiEEKYb"
      },
      "source": [
        "!unzip -q \"archive.zip\" #uzipping dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head shortjokes.csv\n"
      ],
      "metadata": {
        "id": "TiAn7Z5SMXIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "class JokesDataset(Dataset):\n",
        "    def __init__(self, data_path, max_len=1000, append_prefix=\"Generate joke: \"):\n",
        "        self.append_prefix = append_prefix\n",
        "\n",
        "        self.samples = list()\n",
        "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            csv_reader = csv.reader(f)\n",
        "            header = next(csv_reader)\n",
        "            for line in tqdm(csv_reader):\n",
        "                self.samples.append(line)\n",
        "                if len(self.samples) > max_len:\n",
        "                    break\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {}\n",
        "        sample[\"input\"] = (\n",
        "            self.append_prefix\n",
        "            + \" \".join((str(self.samples[idx][1]).split(\" \"))[0:4])\n",
        "        )\n",
        "        sample[\"target\"] = self.samples[idx][1]\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "t97i8ADdOArG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Tuple, List\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "class DataloaderCreator:\n",
        "    \"\"\"\n",
        "    DataloaderCreator creates a dataset and split it into train and val subsets.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path, ratio, batch_size, workers):\n",
        "        self.data_path = data_path\n",
        "        self.ratio = ratio\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "\n",
        "    def _get_split_length(\n",
        "        self, dataset: torch.utils.data.ConcatDataset\n",
        "    ) -> Tuple[int, int]:\n",
        "        train_val_ratio = self.ratio\n",
        "        train_len = round(len(dataset) * train_val_ratio)\n",
        "        val_len = len(dataset) - train_len\n",
        "        return train_len, val_len\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        train = JokesDataset(self.data_path)\n",
        "\n",
        "        train_len, val_len = self._get_split_length(train)\n",
        "\n",
        "        train, val = random_split(\n",
        "            train, [train_len, val_len], generator=torch.Generator().manual_seed(0)\n",
        "        )\n",
        "\n",
        "        dataloader_train = DataLoader(\n",
        "            train,\n",
        "            shuffle=True,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.workers,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        dataloader_val = DataLoader(\n",
        "            val,\n",
        "            shuffle=False,\n",
        "            batch_size=self.batch_size,\n",
        "            num_workers=self.workers,\n",
        "            drop_last=False,\n",
        "        )\n",
        "        return dataloader_train, dataloader_val\n"
      ],
      "metadata": {
        "id": "QEtvvXWPOcgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQHDzPdJGhKB"
      },
      "source": [
        "Now we will seed everything so the results are reproducible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5yau0GSZ5ZU"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything(2021)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAhHPN1l5Bb"
      },
      "source": [
        "## Loading data\n",
        "Now, we will preprocess and load training set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataloaderCreator(\n",
        "        \"shortjokes.csv\",\n",
        "        ratio=0.9,\n",
        "        batch_size=8,\n",
        "        workers=2,\n",
        "    )\n",
        "dataloader_train, dataloader_val = loader.get_dataloaders()\n",
        "\n"
      ],
      "metadata": {
        "id": "YhfM_yIdOmRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader_train:\n",
        "  print(batch['input'])\n",
        "  print(batch['target'])\n",
        "  break"
      ],
      "metadata": {
        "id": "7rZN8sYrS7Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tilJBo7koGdr"
      },
      "source": [
        "## Define the model\n",
        "We define T5ForConditionalGeneration from Transformers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ZjmhSU6IS3"
      },
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import pytorch_lightning as pl\n",
        "from torch.optim.lr_scheduler import MultiplicativeLR\n",
        "\n",
        "\n",
        "class JokeT5(pl.LightningModule):\n",
        "    \"\"\"JokeT5 Model for jokes gneration\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        lr=1e-5,\n",
        "        multiply_lr_step=0.9,\n",
        "        warmup_steps=100.0,\n",
        "        model_path=\"t5-small\",\n",
        "        model_save_dir=\"joke-t5.pkl\",\n",
        "        model_load_dir=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.model_save_dir = model_save_dir\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.multiply_lr_step = multiply_lr_step\n",
        "\n",
        "\n",
        "    def forward(self, input_sequences, output_sequences, **kwargs):\n",
        "        input_sequences = [sequence for sequence in input_sequences]\n",
        "        input_tokens = self.tokenizer(\n",
        "            input_sequences,\n",
        "            padding=True,\n",
        "            truncation=False,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = input_tokens.input_ids\n",
        "        attention_mask = input_tokens.attention_mask\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            output_sequences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "        )\n",
        "        labels = target_encoding.input_ids\n",
        "\n",
        "        # replace padding token id's of the labels by -100\n",
        "        labels = labels = [\n",
        "            [\n",
        "                (label if label != self.tokenizer.pad_token_id else -100)\n",
        "                for label in labels_example\n",
        "            ]\n",
        "            for labels_example in labels\n",
        "        ]\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        loss = self.model(\n",
        "            input_ids=input_ids.to(self.device),\n",
        "            attention_mask=attention_mask.to(self.device),\n",
        "            labels=labels.to(self.device),\n",
        "        ).loss\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_sequences, output_sequences = batch[\"input\"], batch[\"target\"]\n",
        "        loss = self(input_sequences, output_sequences)\n",
        "        self.log(\"loss\", loss, batch_size=1)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        if self.trainer.global_step > 0:\n",
        "            print(\"Saving model...\")\n",
        "            torch.save(self.model.state_dict(), self.model_save_dir)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_sequences, output_sequences = batch[\"input\"], batch[\"target\"]\n",
        "        loss = self(input_sequences, output_sequences)\n",
        "        self.log(\"validation_loss\", loss, batch_size=1)\n",
        "\n",
        "    def validation_epoch_end(self, out):\n",
        "        if self.trainer.global_step > 0:\n",
        "            print(\"Saving model...\")\n",
        "            torch.save(self.model.state_dict(), self.model_save_dir)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "        def lambd(epoch):\n",
        "            return self.multiply_lr_step\n",
        "\n",
        "        scheduler = MultiplicativeLR(optimizer, lr_lambda=lambd)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def optimizer_step(\n",
        "        self,\n",
        "        epoch,\n",
        "        batch_idx,\n",
        "        optimizer,\n",
        "        optimizer_idx,\n",
        "        optimizer_closure,\n",
        "        on_tpu=False,\n",
        "        using_native_amp=False,\n",
        "        using_lbfgs=False,\n",
        "    ):\n",
        "        if self.trainer.global_step < self.warmup_steps:\n",
        "            lr_scale = min(1.0, float(self.trainer.global_step + 1) / self.warmup_steps)\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = lr_scale * self.lr\n",
        "\n",
        "        optimizer.step(closure=optimizer_closure)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itOwE8BMjXrl"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9r6gGCZ6tJQ"
      },
      "source": [
        "model = JokeT5()\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=5,\n",
        "    gpus=[0],\n",
        "    progress_bar_refresh_rate=50,\n",
        "    accumulate_grad_batches=4,\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10,log_every_n_steps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(model,dataloader_train, dataloader_val)"
      ],
      "metadata": {
        "id": "YFE1VNf3R2l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                       "
      ],
      "metadata": {
        "id": "NJ_sqWRtQoIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gZfkO199SZJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}